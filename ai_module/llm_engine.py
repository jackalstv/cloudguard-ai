import subprocess

def generate_summary_with_ai(issues, recommendations, model="llama3.1"):
    """
    Utilise Ollama pour g√©n√©rer un r√©sum√© IA √† partir des vuln√©rabilit√©s d√©tect√©es.
    """
    prompt = f"""
    Voici les vuln√©rabilit√©s d√©tect√©es : {issues}.
    Voici les recommandations techniques : {recommendations}.
    R√©dige une synth√®se en fran√ßais professionnel pour un rapport d‚Äôaudit PME.
    Le ton doit √™tre clair, concis et non technique.
    """

    try:
        # üß† nouvelle syntaxe Ollama : on envoie le prompt via stdin, plus de --prompt
        result = subprocess.run(
            ["ollama", "run", model],
            input=prompt,          # prompt pass√© dans stdin
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=60
        )

        if result.returncode != 0:
            print("‚ùå Erreur Ollama :", result.stderr)
            return "(Erreur : aucun texte g√©n√©r√© par le mod√®le IA.)"

        output = result.stdout.strip()
        if not output:
            output = "(IA locale n'a renvoy√© aucun texte.)"
        return output

    except FileNotFoundError:
        return "(Erreur : Ollama non trouv√©. V√©rifie l'installation avec 'ollama list'.)"
    except subprocess.TimeoutExpired:
        return "(Erreur : temps d'ex√©cution IA d√©pass√©.)"
